[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Multi-Head Attention\n\n\n\ntransformers\n\nmachine-learning\n\n\n\nAn explanation of the Multi-Head Attention layer.\n\n\n\n\n\nSep 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Alter Git Histories\n\n\n\ngit\n\nTIL\n\n\n\nRemove files from your git history with git-filter-repo\n\n\n\n\n\nNov 12, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/attention/index.html",
    "href": "blog/attention/index.html",
    "title": "Multi-Head Attention",
    "section": "",
    "text": "In this blog post I will give my explanation of the multi-head attention mechanism, which is the core ingredient for the transformer architecture which underpins the latest large-language models (LLMs) such as chatGPT and Claude.\nThe multi-head attention layer is parameterized by"
  },
  {
    "objectID": "blog/attention/index.html#one-attention-head",
    "href": "blog/attention/index.html#one-attention-head",
    "title": "Multi-Head Attention",
    "section": "One attention head",
    "text": "One attention head\nLet’s start with the case of one attention head. The generalization to multiple heads will be straightforward after this.\nThe attention layer in this case is a function\n\\[\n\\operatorname{Attention}_{S, I, \\mathcal{O}, D}: \\mathbb{R}^{(S, I)} \\to \\mathbb{R}^{(S, \\mathcal{O})}\n\\]\nwhere \\(\\mathbb{R}^{(S, I)}\\) is the vector space of arrays of shape \\((S, I)\\). This says that the input to attention is an array \\(X\\) of shape \\((S, I)\\) and the output is an array of shape \\((S, \\mathcal{O})\\).\nThe \\(S\\) axis can be thought of in various ways. When used in a language model, the \\(S\\) axis corresponds to position along a text input; in a time series model the \\(S\\) axis is the time axis.\nThe \\(I\\) axis represents the input features or channels. In a language model, each word in some input text gets mapped to an embedding vector of dimension \\(I\\), which is nicely explained in this 3Blue1Brown video. In a time series model, you might have several different signals that are measured at each point in time, and the positions along the \\(I\\) axis represent those different signals.\nAn input array \\(X \\in \\mathbb{R}^{(S, I)}\\) can thus be thought of as a sequence of \\(I\\)-dimensional vectors \\((x_p)\\) indexed by the sequence axis \\(S\\). With this convention, the \\(S\\)-axis goes along the rows and the \\(I\\)-dimensional vectors are row vectors.\n\n\n\n\n\n\nFigure 1: Attention input tensor of shape \\((S, I)\\)\n\n\n\nThe formula for attention is\n\\[\n\\begin{align}\n\\operatorname{Attention}(X) &=\n  \\operatorname{softmax}\\bigg(\n    \\frac{1}{\\sqrt{D}}\n    QK^T\n  \\bigg) V\n\\end{align}\n\\tag{1}\\]\nwhere \\(Q, K, V\\) are called queries, keys, and values respectively. These matrices are defined as\n\\[\n\\begin{align}\n  Q &= XW_Q + b_Q \\\\\n  K &= XW_K + b_K \\\\\n  V &= XW_V + b_V\n\\end{align}\n\\tag{2}\\]\nThe above formulas use matrix multiplication and the sum of the bias vector is broadcasted across the sequence axis. By matrix multiplication we can write this as an equation on the rows of \\(Q\\), \\(K\\), and \\(V\\). For example, for the rows \\((q_p)\\) of \\(Q\\) we have\n\\[q_p = x_p W_Q + b_Q \\tag{3}\\]\n(and similarly for rows \\((k_p)\\) of \\(K\\) and \\((v_p)\\) of \\(V\\)). If this equation looks a little strange it’s because it’s an equation on row vectors, whereby matrices act on the right instead of the left. A consequence of this is that the feature rows of \\(Q, K, V\\) are completely localized to their position in the sequence dimension; they do not contain information from other locations along the input sequence.\nThe shapes of these arrays are summarized in the following table. Note that \\(Q\\), \\(K\\), and \\(V\\) can be visualized similarly to \\(X\\) as in Figure 1 as sequences of vectors along the \\(S\\)-axis.\n\n\n\n\n\nArray\nShape\n\n\n\n\n\\(W_Q\\)\n\\((I, D)\\)\n\n\n\\(W_K\\)\n\\((I, D)\\)\n\n\n\\(W_V\\)\n\\((I, \\mathcal{O})\\)\n\n\n\\(Q\\)\n\\((S, D)\\)\n\n\n\\(K\\)\n\\((S, D)\\)\n\n\n\\(V\\)\n\\((S, \\mathcal{O})\\)\n\n\n\n\n\nEach of the weights \\((W, b)\\) are free parameters of the layer that are learned during model training.\n\nAttention Scores\nThe next step towards Equation 1 is the computation of the attention scores, which are given by the matrix\n\\[\nA = \\operatorname{softmax}\\bigg(\n  \\frac{1}{\\sqrt{D}}\n  QK^T\n\\bigg)\n\\tag{4}\\]\nappearing in Equation 1. Starting from the inside, the matrix \\(QK^T\\) is the matrix of dot-products between all query vectors and all key vectors.\n\\[\nQ K^T =\n\\begin{pmatrix}\n\\textemdash q_1 \\textemdash \\\\\n\\textemdash q_2 \\textemdash \\\\\n\\textemdash q_3 \\textemdash \\\\\n\\vdots\n\\end{pmatrix}\n\\begin{pmatrix}\n     |&     | &     | & \\\\\nk_1^T & k_2^T & k_3^T & \\cdots \\\\\n     |&     | &     | & \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\nq_1k_1^T & q_1k_2^T & \\cdots & &\\\\\nq_2k_1^T & q_2k_2^T & \\cdots & & \\\\\n\\vdots   & \\vdots   & \\ddots & & \\\\\n         &          &        & q_ik_j^T & \\\\\n         &          &        &          & \\ddots \\\\\n\\end{pmatrix}\n\\in \\mathbb{R}^{(S, S)}\n\\]\nA dot product \\(q_ik_j^T\\) will be small when the query and key columns are very different, and large when the they are nearly parallel. For this reason you can think of the queries as retrieving information by “looking up” the keys using dot products. When the query aligns with the key, that key gets selected, and we might say that position \\(i\\) attends to position \\(j\\).\nWe can start to see that the attention mechanism is like a fuzzy dictionary. A query \\(q_i\\) at position \\(i\\) in the sequence “looks” at all keys \\(k_j\\) along the sequence by taking the dot product with them. When that has high alignment, the associated value \\(v_j\\) to \\(k_j\\) will then contribute to the final output of the attention layer at position \\(i\\).\nTo understand the fuzzy part, we must continue digesting Equation 4. I’ll come back to the \\(\\sqrt{D}\\) in a moment, so let’s understand what’s going on with the softmax.\nIf we didn’t have the softmax around, attention would output \\(AV= QK^T V\\):\n\\[\n\\begin{align*}\nQK^T V &= \\begin{pmatrix}\nq_1k_1^T & q_1k_2^T & \\cdots & &\\\\\nq_2k_1^T & q_2k_2^T & \\cdots & & \\\\\n\\vdots   & \\vdots   & \\ddots & & \\\\\n         &          &        & q_ik_j^T & \\\\\n         &          &        &          & \\ddots \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n  \\textemdash v_1 \\textemdash \\\\\n  \\textemdash v_2 \\textemdash \\\\\n  \\vdots\\\\\n  \\textemdash v_j \\textemdash \\\\\n  \\vdots\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix}\n  (q_1k_1^T)v_1 + (q_1k_2^T)v_2 + \\cdots + (q_1k_S^T)v_S \\\\\n  (q_2k_1^T)v_1 + (q_2k_2^T)v_2 + \\cdots + (q_2k_S^T)v_S \\\\\n  \\cdots\\\\\n  (q_ik_1^T)v_1 + (q_ik_2^T)v_2 + \\cdots + (q_ik_S^T)v_S \\\\\n  \\cdots\n\\end{pmatrix} \\overset{\\text{def}}{=}\n\\begin{pmatrix}\n  \\textemdash o_1 \\textemdash \\\\\n  \\textemdash o_2 \\textemdash \\\\\n  \\cdots \\\\\n  \\textemdash o_i \\textemdash \\\\\n  \\cdots\n\\end{pmatrix}\n\\end{align*}\n\\]\nThe result1 is a little messy, but it shows what is meant by “fuzzy”: the output vector \\(o_i\\) at position \\(i\\) in the sequence is a linear combination of the value vectors, and each value \\(v_j\\) contributes by an amount proportional to the lookup weights \\(q_ik_j^T\\).\nAdding the softmax and factor of \\(\\sqrt{D}\\) back in doesn’t alter this core idea. The softmax in Equation 5 is applied over the rows of \\(QK^T\\), so it converts the set of lookup weights for each output to a probability distribution:\n\\[\n\\operatorname{softmax}(q_ik_1^T/\\sqrt{D}, q_ik_2^T/\\sqrt{D}, \\dots, q_ik_S^T/\\sqrt{D})\n  \\mapsto (a_{i,1}, a_{i, 2}, \\dots, a_{i, S})\n\\]\nwhere\n\\[\n\\begin{align}\n  a_{i, j} = \\frac{\\exp(q_ik_j^T/\\sqrt{D})}{\\sum_{l} \\exp(q_ik_l^T/\\sqrt{D})}.\n\\end{align}\n\\tag{5}\\]\nare the attention scores. The output array’s \\(i^{\\text{th}}\\) row still looks like a linear combination of value vectors:\n\\[\n  o_i = a_{i, 1}v_1 + a_{i, 2}v_2 + \\dots a_{i, S}v_S.\n\\tag{6}\\]\nNow, it’s not the lookup weight \\(q_ik_j^T\\) but rather a scaled version2 of its exponential in Equation 5 that tells us how much \\(v_j\\) contributes to \\(o_i\\). We get a maximal contribution when \\(q_i\\) is parallel to \\(k_j\\), and a minimal contribution when they are anti-parallel.\n\nNormalizing Factor \\(\\sqrt{D}\\)\nThe normalizing factor of \\(\\sqrt{D}\\) in these equations is to make sure that the softmax operation doesn’t become oversaturated. Roughly what happens is the lookup weights \\(q_ik_j^T\\) have a variance equal to \\(D\\), so you can expect that several of them are much larger than others. Due to the exponentials in the definition of softmax in Equation 5, these much larger values dominate the softmax and most other weights are near \\(0\\). Scaling by \\(\\sqrt{D}\\) brings the variance down to 1, and that ensures that it is unlikely for one particular lookup weight to dominate.\nFor more details, see this great stackexchange answer."
  },
  {
    "objectID": "blog/attention/index.html#multiple-heads",
    "href": "blog/attention/index.html#multiple-heads",
    "title": "Multi-Head Attention",
    "section": "Multiple Heads",
    "text": "Multiple Heads\nGeneralizing to multiple heads is straightforward: if \\(N\\) is the number of heads, you chop up the model dimension \\(D\\) into \\(N\\) bins and apply the attention formula Equation 1 \\(N\\) times, using \\(D_{head} = D / N\\) in place of \\(D\\) and \\(\\mathcal{O}_{head} = \\mathcal{O} / N\\) in place of \\(\\mathcal{O}\\).\nFor each \\(n = 1, 2, \\dots, N\\), the \\(n\\)th attention head outputs an array of shape \\((S, \\mathcal{O}_{head})\\) whose rows are \\(o_i^{(n)}\\). Each of the \\(o_i^{(n)}\\) have the same mathematical form as Equation 6, except all the row vectors \\(q, k, v\\) of each matrix have reduced length (which means \\(o_i^{(n)}\\) also has reduced length because it’s a sum of \\(v\\)’s). They then fit together to form the final output vector \\(o_i\\):\n\\[\no_i = \\begin{pmatrix}\n  o_i^{(1)} & \\bigg| & o_i^{(2)} & \\bigg| & \\cdots & \\bigg| & o_{i}^{(N)}\n\\end{pmatrix}\n\\tag{7}\\]\nWhen allowing for more heads, we are deciding that our output features are grouped into sub-outputs, each of which has an attention head behind it. These different groups are then free to learn about different features of the input array \\(X\\). This is similar to how a convolutional layer with multiple features can learn to pick out distinct components of the input signal."
  },
  {
    "objectID": "blog/attention/index.html#positional-encodings",
    "href": "blog/attention/index.html#positional-encodings",
    "title": "Multi-Head Attention",
    "section": "Positional Encodings",
    "text": "Positional Encodings\nSo at this point we’ve fully defined the attention layer; the output array has rows \\(o_i\\) given by Equation 6 (or Equation 7 when multiple heads are used).\nHowever, everything that goes into the attention scores \\(a_{i,j}\\) is purely based on the values at each position, but it does not make use of the relative position of each query and key. Basically, we’re only looking at how values at \\(i\\) compare to values at \\(j\\), but we haven’t actually included information about how close \\(i\\) is to \\(j\\).\nTo illustrate this a bit a further, consider the following sentence.\n\nThey decided to park the car in the park.\n\nBy our construction, queries will attend to the word “park” at both occurrences with equal attention, because both “park”s will have the same key and value vectors. The missing context is how far apart the words are; a good model would probably learn to attend differently to the same word at different locations in the sentence.\nOne solution is to add positional encoding to the sequence at some point before we calculate attention scores. I’ll briefly mention two methods in this post: fixed vector encodings and rotational encodings. There are many variants of these out there, and these two are relatively common.\n\nFixed Vecors\nThe easiest way to add positional encoding is to add on a fixed vector at each point along the sequence. We take vectors \\(\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_S\n\\in \\mathbb{R}^I\\) and use \\(x_p + \\epsilon_p\\) instead of x_p$ in Equation 3, and allow these vectors to be learnable parameters during training. This was the strategy used in the original paper. The method is pretty simple, but it turns out to not lead to the best results.\nThis method also lacks translation invariance. The transformer only knows to distinguish positions based on the raw values of the \\(\\epsilon_i\\), and if you were to shift your input sequence you might get different results due to the shifted input matching up with different \\(epsilon_i\\).\n\n\nRotational Encodings\nA more sophisticated approach appearing the Roformer paper involves rotating the queries \\(Q\\) and keys \\(K\\) by rotations \\(R_1, R_2, ..., R_S\\) such that any neighboring rotations differ by the same angle \\(\\theta\\). This breaks the symmetry we observed above, and moreover it is less sensitive to translations, because the relative rotation is preserved along the sequence. This paper has shown this to be a more effective method of positional encoding than the fixed vector approach."
  },
  {
    "objectID": "blog/attention/index.html#footnotes",
    "href": "blog/attention/index.html#footnotes",
    "title": "Multi-Head Attention",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that in general, you cannot turn the parentheses around to write \\(o_i = q_i(k_1^Tv_1 + \\cdots + k_S^Tv_S)\\), because unless \\(D =\n\\mathcal{O}\\), the dot product \\(k_j^T v_j\\) does not make sense.↩︎\nNote that the denominator of the softmax values \\(a_{i,j}\\) is the same for all \\(j\\).↩︎"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Moduli Spaces of Twisted Hermite-Einstein Connections on K3 Surfaces. My PhD thesis under the supervision of Nick Addington.\nTwisted Fourier-Mukai partners of Enriques surfaces. With Nick Addington. Appears in Math. Z. 297, 2021. (Arxiv Link)"
  },
  {
    "objectID": "research.html#in-mathematics",
    "href": "research.html#in-mathematics",
    "title": "Research",
    "section": "",
    "text": "Moduli Spaces of Twisted Hermite-Einstein Connections on K3 Surfaces. My PhD thesis under the supervision of Nick Addington.\nTwisted Fourier-Mukai partners of Enriques surfaces. With Nick Addington. Appears in Math. Z. 297, 2021. (Arxiv Link)"
  },
  {
    "objectID": "research.html#in-physics",
    "href": "research.html#in-physics",
    "title": "Research",
    "section": "In Physics",
    "text": "In Physics\n\nHyperspherical approach to a three-boson problem in two dimensions with a magnetic field. With Seth Rittenhouse and B.L. Johnson. Appears in Phys. Rev. A 93, 2016."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andrew Wray",
    "section": "",
    "text": "I’m Andrew Wray, and I am a data scientist and mathematician based in Seattle, WA.\nI received my Bachelor’s of Science in Math and Physics at Western Washington University in 2014, where I was recognized as WWU Math Department’s Outstanding Graduate. I completed my PhD in mathematics at the University of Oregon in June 2020 under the guidance of Nick Addington in the field of algebraic geometry. These days I work as a data scientist for Fullpower Technologies.\nIf you wish to get in contact with me, feel free to send me an email (andrew.wray2718 at gmail dot com) or reach out on LinkedIn."
  },
  {
    "objectID": "blog/til/git-filter-repo/index.html",
    "href": "blog/til/git-filter-repo/index.html",
    "title": "How to Alter Git Histories",
    "section": "",
    "text": "I recently cleaned a couple of git repos that had large data files committed early in their history, and in the process I learned about git-filter-repo, a tool for cleanly altering git histories.\nThere are many reasons you might need to modify your git history. For example, consider a local repo you want to push to Github that at some point in time had a file larger than their 100MB cap committed. In order to push to Github, you would need to not only remove the file from your repo with git rm, but also remove the file from any commit it showed up in. Another common scenario: you want to purge your git history of any accidentally tracked junk files, such as __pycache__ folders or .DS_Store files.\nIn both scenarios, the goal becomes to completely rid a file (or directory) from the git history."
  },
  {
    "objectID": "blog/til/git-filter-repo/index.html#the-old-way-git-filter-branch",
    "href": "blog/til/git-filter-repo/index.html#the-old-way-git-filter-branch",
    "title": "How to Alter Git Histories",
    "section": "The old way: git filter-branch",
    "text": "The old way: git filter-branch\nWhen you search around for ideas on how to rid files from histories you might find a lot of older stack-exchange posts and tutorial websites with solutions involving git filter-branch. However, according to the git filter-repo readme, filter-branch has numerous problems: it is slow, potentially unsafe for your repository, and clunky to use. For that reason I won’t describe how to use it here."
  },
  {
    "objectID": "blog/til/git-filter-repo/index.html#enter-git-filter-repo",
    "href": "blog/til/git-filter-repo/index.html#enter-git-filter-repo",
    "title": "How to Alter Git Histories",
    "section": "Enter git filter-repo",
    "text": "Enter git filter-repo\nPeople have since built simpler, more effective tools for performing git history manipulations, and the best one I’ve found is git-filter-repo. I picked it after having been convinced by their comparisons to other tools in this area. They cover many use cases in their handbook, which is worth at least glancing over."
  },
  {
    "objectID": "blog/til/git-filter-repo/index.html#example-removing-files-from-the-git-history",
    "href": "blog/til/git-filter-repo/index.html#example-removing-files-from-the-git-history",
    "title": "How to Alter Git Histories",
    "section": "Example: removing files from the git history",
    "text": "Example: removing files from the git history\nIn this post I’ll focus on the example of removing a file from the git history. However, this works the same with directories and similarly with glob patterns or regex; see --path-glob and --path-regex.\nFor illustration, I’ll initialize an empty git repository and add two files, file_1.txt and file_2.txt, in a single commit.\n\nmkdir /tmp/new-repo && cd /tmp/new-repo\ngit init\ntouch file_1.txt file_2.txt\ngit add .\ngit commit -m \"Initial commit\"\n\nInitialized empty Git repository in /private/tmp/new-repo/.git/\n[main (root-commit) 4d23f8d] Initial commit\n 2 files changed, 0 insertions(+), 0 deletions(-)\n create mode 100644 file_1.txt\n create mode 100644 file_2.txt\n\n\n\ngit log --name-status --oneline\n\n\n4d23f8d (HEAD -&gt; main) Initial commit\n\nA   file_1.txt\n\nA   file_2.txt\n\n\n\n\nFor this example our plan will be to delete file_2.txt from the git history without deleting file_2.txt itself.\n\nStep 1: Decaching\n“Decaching” is a word I made up for this step of “remove the file from the current git commit but keep it around in the folder.” You can do this with\n\ngit rm --cached file_2.txt\n\nrm 'file_2.txt'\n\n\nVerify by checking that ls still shows both files, and that git status shows that file_2.txt is no longer tracked.\n\nls\n\nfile_1.txt  file_2.txt\n\n\n\ngit status --short\n\n\n## main\n\nD  file_2.txt\n\n?? file_2.txt\n\n\n\n\n\n\nStep 2: filter-repo\nWith file_2.txt unstaged, apply filter-repo like this to delete file_2.txt from the history:\n\ngit filter-repo --path file_2.txt --invert-paths --force\n\nParsed 1 commits\nNew history written in 0.02 seconds; now repacking/cleaning...\nRepacking your repo and cleaning out old unneeded objects\nHEAD is now at 9e2acce Initial commit\nEnumerating objects: 3, done.\nCounting objects:  33% (1/3)Counting objects:  66% (2/3)Counting objects: 100% (3/3)Counting objects: 100% (3/3), done.\nWriting objects:  33% (1/3)Writing objects:  66% (2/3)Writing objects: 100% (3/3)Writing objects: 100% (3/3), done.\nTotal 3 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\nCompletely finished after 0.07 seconds.\n\n\nThe --path specifies the path you’re trying to target for removal, and the --invert-paths is basically the logical negation of the filtering condition, so when it’s applied it will only delete file_2.txt. When you leave that flag off, you instead delete everything except file_2.txt. You get only the file, or everything but the file.\nThe --force flag is needed because filter-repo expects us to follow best practices by only using it on a fresh clone. In practice1, you would commit all your work, get a clean git state, and make a fresh clone of your repo to operate on with filter-repo.\nNow check the files in the git log and filesystem:\n\nls\n\nfile_1.txt  file_2.txt\n\n\n\ngit log --name-status --oneline                         \n\n\n9e2acce (HEAD -&gt; main) Initial commit\n\nA   file_1.txt\n\n\n\n\nThe single commit now does not have any information pertaining to file_2.txt, and file_2.txt is still around on the filesystem. (If you just wanted to delete it completely, you could just skip the decaching step altogether.)\nThat’s all there is to it!"
  },
  {
    "objectID": "blog/til/git-filter-repo/index.html#footnotes",
    "href": "blog/til/git-filter-repo/index.html#footnotes",
    "title": "How to Alter Git Histories",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee this part of the handbook↩︎"
  }
]